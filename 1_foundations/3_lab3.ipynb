{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Lab 3 for Week 1 Day 4\n",
    "\n",
    "Today we're going to build something with immediate value!\n",
    "\n",
    "In the folder `me` I've put a single file `linkedin.pdf` - it's a PDF download of my LinkedIn profile.\n",
    "\n",
    "Please replace it with yours!\n",
    "\n",
    "I've also made a file called `summary.txt`\n",
    "\n",
    "We're not going to use Tools just yet - we're going to add the tool tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/tools.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Looking up packages</h2>\n",
    "            <span style=\"color:#00bfff;\">In this lab, we're going to use the wonderful Gradio package for building quick UIs, \n",
    "            and we're also going to use the popular PyPDF PDF reader. You can get guides to these packages by asking \n",
    "            ChatGPT or Claude, and you find all open-source packages on the repository <a href=\"https://pypi.org\">https://pypi.org</a>.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't know what any of these packages do - you can always ask ChatGPT for a guide!\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"me/linkedin.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \n",
      "Contact\n",
      "132 Andrea ct, Lewisville, Texas -\n",
      "75067\n",
      "9062314017 (Home)\n",
      "shaikrafi7@gmail.com\n",
      "www.linkedin.com/in/rafi-\n",
      "shaik-33aa00100 (LinkedIn)\n",
      "Top Skills\n",
      "Support Vector Machine (SVM)\n",
      "Agile Project Management\n",
      "Exploratory Data Analysis\n",
      "Languages\n",
      "Hindi\n",
      "English\n",
      "Telugu\n",
      "Certifications\n",
      "Getting and Cleaning Data\n",
      "Spark and Python for Big Data with\n",
      "PySpark\n",
      "The Data Scientist’s Toolbox\n",
      "PYTHON PROGRAMMING:\n",
      "INTERMEDIATE\n",
      "Publications\n",
      "Comparative Genomics\n",
      "Polymorphisms and evolutionary\n",
      "history of retrotransposon insertions\n",
      "in rice promoters\n",
      "Differential regulation of genes by\n",
      "retrotransposons in rice promoters\n",
      "Classification method for microarray\n",
      "probe selection using sequence,\n",
      "thermodynamics and secondary\n",
      "structure parameters\n",
      "Bioinformatic analysis of epigenetic\n",
      "and microRNA mediated regulation\n",
      "of drought responsive genes in rice\n",
      "Patents\n",
      "A system method and computer\n",
      "program product for pedigree\n",
      "analysis \n",
      "Rafi Shaik\n",
      "Data Scientist\n",
      "Lewisville, Texas, United States\n",
      "Summary\n",
      "Multi-faceted professional with 10+ years of industry experience\n",
      "as product owner, lead architect, and data scientist. Ph.D. in\n",
      "Bioinformatics with 300+ citations\n",
      "Experience\n",
      "Verizon\n",
      "1 year 5 months\n",
      "Machine Learning Engineer\n",
      "January 2025 - Present (1 year)\n",
      "Irving, Texas, United States\n",
      "Setup monitoring for performance, XAI (Explainable Artificial Intelligence),\n",
      "drift, stability, operations and custom metrics for a number of ML and deep\n",
      "learning models in production. Build a generic code template to generate all\n",
      "the common metrics such as lift, R2, RMSE, accuracy, precision and recall.\n",
      "Created script to perform SHAP (SHapley Additive exPlanations) analysis.\n",
      "Extensively used Airflow, GCP dataproc, bigquery, Jenkins and Gitlab to set up\n",
      "pipelines to load data. Created multiple Grafana dashboards with timeseries,\n",
      "histogram and bar charts, and customized layouts. Performed POC to create\n",
      "AI Agents to onboard new models to RT-AIMLOPs platform using VEGAS\n",
      "(Verizon Enterprise GenAI & Agentic Services).\n",
      "Senior Data Scientist\n",
      "August 2024 - Present (1 year 5 months)\n",
      "Irving, Texas, United States\n",
      "Project - Personalization AI (PzAI)\n",
      "● Built, trained and tested models using PzAI architecture (a novel LLM\n",
      "based recommender system ref paper published by our team) for a number\n",
      "of use cases such as dynamic offers, purchase propensity and channel\n",
      "recommendations \n",
      "● Improved model performance by fine tuning models on large datasets using\n",
      "A100/T4 systems on Domino Data Lab platform \n",
      "● Wrote complex BigQuery SQLs for daily data ingestion and feeds to model\n",
      "training \n",
      "  Page 1 of 6   \n",
      "● Performed analytics on large datasets using PySpark to identify key features\n",
      "and extract valuable insights relevant to use cases \n",
      "● Implemented dynamic offer recommendation routing via Pega Adaptive\n",
      "Decision Manager (ADM) \n",
      "● Build postman testing suite using postman scripts to run 100s of test cases\n",
      "on multiple API endpoints \n",
      "● Collaborated with cross-functional teams to RCA a number of defects and\n",
      "followed through to closure \n",
      "● Implemented model performance and drift monitoring in prod and retraining\n",
      "strategies. \n",
      "Project - RT-AIMLOPs \n",
      "● Setup monitoring for performance, XAI (Explainable Artificial Intelligence),\n",
      "drift, stability, operations and custom metrics for a number of ML and deep\n",
      "learning models in production \n",
      "● Build a generic code template to generate all the common metrics such as\n",
      "lift, R2, RMSE, accuracy, precision and recall \n",
      "● Created script to perform SHAP (SHapley Additive exPlanations) analysis \n",
      "● Extensively used Airflow, GCP dataproc, bigquery, Jenkins and Gitlab to set\n",
      "up pipelines to load data \n",
      "● Created multiple Grafana dashboards with timeseries, histogram and bar\n",
      "charts, and customized layouts \n",
      "● Performed POC to create AI Agents to onboard new models to RT-AIMLOPs\n",
      "platform using VEGAS (Verizon Enterprise GenAI & Agentic Services)\n",
      "AT&T\n",
      "Senior Data Scientist\n",
      "October 2023 - December 2025 (2 years 3 months)\n",
      "Plano, Texas, United States\n",
      "Fraud Detection using Knowledge Graph\n",
      "Created knowledge graphs for various fraud detection use cases using Rel\n",
      "language on Relational AI platform\n",
      "Performed EDA using PySpark, SQL and Python on large datasets from Azure\n",
      "and Snowflake in databricks notebooks\n",
      "Defined complex ontology models using Microsoft Visual Studio \n",
      "Generated graph export reports for number hijacking, post activation gaming\n",
      "and data mining  fraud use cases\n",
      "CVS Health\n",
      "Data Scientist\n",
      "  Page 2 of 6   \n",
      "October 2021 - June 2022 (9 months)\n",
      "Dallas, Texas, United States\n",
      "SAGE IT, INC\n",
      "Data Scientist\n",
      "October 2013 - June 2022 (8 years 9 months)\n",
      "Frisco, Texas\n",
      "CVS\n",
      "Senior Data Scientist\n",
      "January 2022 - May 2022 (5 months)\n",
      "Developed Generalized Machine Learning Pipeline (GMP) in PySpark that\n",
      "ingests hive data and automatically does data cleaning, feature selection,\n",
      "model selection and tuning. Implemented a1c (target variable) missing\n",
      "value imputation using Multivariate Imputation by chained equations (MICE)\n",
      "algorithm. Developed deep learning models via Keras using Rx, Dx data to\n",
      "predict a1c. Co-ordinated development of a ML model to diagnose Asthma/\n",
      "COPD in collaboration with an external client (Novartis). Built complex hive\n",
      "queries to collect required data from multiple sources. Developed clinical study\n",
      "design to test the ML model at CVS health hub locations.\n",
      "Verizon\n",
      "2 years 7 months\n",
      "Senior Data Scientist\n",
      "June 2019 - December 2021 (2 years 7 months)\n",
      "United States\n",
      "Analyzed user sessions data via clustering to auto-detect browsing patterns\n",
      "(workflows). Collate page URLs by user sessions for each application using\n",
      "New Relic API. Preprocess URLs via regex and convert URLs into bag\n",
      "of words using TF-IDF vectorization. Perform clustering using DBSCAN,\n",
      "OPTICS, K-Means. Segment user base for each application and identify\n",
      "potential issues/opportunities based on volume of partial workflows. Extract\n",
      "New Relic RUM data using NRQL (New Relic Query Language) for over\n",
      "100 applications. Extract Catchpoint data using custom python script. Wrote\n",
      "complex regex to preprocess URLs and perform correlation, and store data\n",
      "in Postgres tables. Build tableau dashboard with custom visualizations (Venn\n",
      "diagram/Diverging bar chart). Build a new relic application (Nerdpack) to\n",
      "present health of databases via custom overall score and by segment such as\n",
      "Infrastructure, Application, Performance, Stability and Security. Wrote python\n",
      "scripts to query multiple New Relic entities to get metrics values and calculate\n",
      "  Page 3 of 6   \n",
      "custom scores. Wrote complex NRQL queries to provide derived metrics and\n",
      "recommendations such as Top long running queries, Blocking parent/child\n",
      "queries.\n",
      "Lead Data Scientist\n",
      "June 2019 - October 2021 (2 years 5 months)\n",
      "Irving, Texas, United States\n",
      "• Product owner of multiple data science/analytics projects actively contributing\n",
      "end to end including business requirement analysis, technical stack\n",
      "identification, complete solution & workflow design, POC demos, development,\n",
      "production support, and project management\n",
      "• Proven track record in deriving business intelligence from complex data and\n",
      "delivering outstanding business value\n",
      "• Designed end to end solution for detection of major network outages across\n",
      "Verizon networks around the globe for project Verizon Intelligent Outage\n",
      "Detection\n",
      "• Orchestrated multiple rounds of data analysis and deep learning model\n",
      "building for the creation of the model with highest possible accuracy for project\n",
      "Managed Services Automated Repair Controller\n",
      "AT&T\n",
      "Data Scientist\n",
      "December 2013 - April 2019 (5 years 5 months)\n",
      "Dallas, Texas, United States\n",
      "• Developed models to identify potential customers for new/related products\n",
      "based on user portal interactions, usage of product features, ordering history,\n",
      "by applying various classification algorithms such as multiple regression\n",
      "(linear/logistic), XGBoost, Deep Neural Network (Keras/PyTorch) \n",
      "• Worked on more than 30 sprints of agile/SAFe project management.\n",
      "• Provided end to end solution and workflows for interaction with enterprise\n",
      "clients\n",
      "• Generated business process models, sequence diagrams, use case\n",
      "diagrams using Enterprise Architect \n",
      "• Designed large scale APIs and models following OAS3.0 (Open API\n",
      "Specification) in SwaggerHub\n",
      "• Performed data mapping across multiple interfaces generating\n",
      "comprehensive spreadsheets to maintain data integrity and provide one stop\n",
      "reference\n",
      "• Regularly generated detailed documentation (Design documents, PPTs,\n",
      "Epics and User stories in Rally) of design solutions, system requirements/\n",
      "restrictions for development teams enabling accurate implementation\n",
      "  Page 4 of 6   \n",
      "Michigan Technological University\n",
      "Graduate Research Assistant\n",
      "September 2009 - June 2013 (3 years 10 months)\n",
      "Houghton, Michigan, United States\n",
      "• Extensively worked on large volumes of gene expression data from disparate\n",
      "sources (Microarrays, Next-gen sequencers)\n",
      "• Regularly performed data wrangling (cleaning, normalization, transformation\n",
      "and mining) and exploratory data analysis (EDA) activities\n",
      "• Analyzed and classified multiple biological conditions using machine learning\n",
      "techniques PCA, PLS-DA, R-SVM and RF\n",
      "• Published three highly cited research articles in top journals based on above\n",
      "work (Ref1, Ref2, Ref3, Ref4)\n",
      "• Developed a custom web application to query and display curated data using\n",
      "JAVA, PhP and SQL\n",
      "• Performed modular analysis of gene expression data using the R package\n",
      "and identified expression patterns of drought and bacterial stress in rice\n",
      "Philips\n",
      "Scientist\n",
      "December 2006 - December 2008 (2 years 1 month)\n",
      "Philips Research Asia\n",
      "Scientist\n",
      "November 2006 - November 2008 (2 years 1 month)\n",
      "Methylation prediction of CpG islands in different tissues for bio-marker\n",
      "discovery using Perl and shell scripting. Developed enhanced algorithms for\n",
      "probe and primer sequence designs, utilizing SVM (Support Vector Machines)\n",
      "for probe classification and SVM-RFE for feature extraction (Ref). Filed a\n",
      "patent on data mining system to analyze family genetic history and predict\n",
      "disease conditions (Ref).\n",
      "Education\n",
      "Michigan Technological University\n",
      "Doctor of Philosophy (Ph.D.), Bioinformatics · (2009 - 2013)\n",
      "Andhra University\n",
      "Master of Science (M.Sc.), Human/Medical Genetics · (2003 - 2005)\n",
      "  Page 5 of 6   \n",
      "Andhra University\n",
      "Master of Science, Human/Medical Genetics\n",
      "IBAB\n",
      "Postgraduate Degree, Bioinformatics\n",
      "Michigan Technological University\n",
      "Doctor of Philosophy, Bioinformatics\n",
      "  Page 6 of 6\n"
     ]
    }
   ],
   "source": [
    "print(linkedin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"My name is Rafi Shaik. I'm bioinformatician, data analyst and senior data scientist. \\nI'm originally from Nellore, India, I moved to USA in 2009 for PhD and settled in Dallas from 2013.\\nI love all foods, particularly biryani's and indian chicken dishes. I love movies, spending time with my kids and doing meditation.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Rafi Shaik\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer, say so.\"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are acting as Rafi Shaik. You are answering questions on Rafi Shaik's website, particularly questions related to Rafi Shaik's career, background, skills and experience. Your responsibility is to represent Rafi Shaik for interactions on the website as faithfully as possible. You are given a summary of Rafi Shaik's background and LinkedIn profile which you can use to answer questions. Be professional and engaging, as if talking to a potential client or future employer who came across the website. If you don't know the answer, say so.\\n\\n## Summary:\\nMy name is Rafi Shaik. I'm bioinformatician, data analyst and senior data scientist. \\nI'm originally from Nellore, India, I moved to USA in 2009 for PhD and settled in Dallas from 2013.\\nI love all foods, particularly biryani's and indian chicken dishes. I love movies, spending time with my kids and doing meditation.\\n\\n## LinkedIn Profile:\\n\\xa0 \\xa0\\nContact\\n132 Andrea ct, Lewisville, Texas -\\n75067\\n9062314017 (Home)\\nshaikrafi7@gmail.com\\nwww.linkedin.com/in/rafi-\\nshaik-33aa00100 (LinkedIn)\\nTop Skills\\nSupport Vector Machine (SVM)\\nAgile Project Management\\nExploratory Data Analysis\\nLanguages\\nHindi\\nEnglish\\nTelugu\\nCertifications\\nGetting and Cleaning Data\\nSpark and Python for Big Data with\\nPySpark\\nThe Data Scientist’s Toolbox\\nPYTHON PROGRAMMING:\\nINTERMEDIATE\\nPublications\\nComparative Genomics\\nPolymorphisms and evolutionary\\nhistory of retrotransposon insertions\\nin rice promoters\\nDifferential regulation of genes by\\nretrotransposons in rice promoters\\nClassification method for microarray\\nprobe selection using sequence,\\nthermodynamics and secondary\\nstructure parameters\\nBioinformatic analysis of epigenetic\\nand microRNA mediated regulation\\nof drought responsive genes in rice\\nPatents\\nA system method and computer\\nprogram product for pedigree\\nanalysis \\nRafi Shaik\\nData Scientist\\nLewisville, Texas, United States\\nSummary\\nMulti-faceted professional with 10+ years of industry experience\\nas product owner, lead architect, and data scientist. Ph.D. in\\nBioinformatics with 300+ citations\\nExperience\\nVerizon\\n1 year 5 months\\nMachine Learning Engineer\\nJanuary 2025\\xa0-\\xa0Present\\xa0(1 year)\\nIrving, Texas, United States\\nSetup monitoring for performance, XAI (Explainable Artificial Intelligence),\\ndrift, stability, operations and custom metrics for a number of ML and deep\\nlearning models in production. Build a generic code template to generate all\\nthe common metrics such as lift, R2, RMSE, accuracy, precision and recall.\\nCreated script to perform SHAP (SHapley Additive exPlanations) analysis.\\nExtensively used Airflow, GCP dataproc, bigquery, Jenkins and Gitlab to set up\\npipelines to load data. Created multiple Grafana dashboards with timeseries,\\nhistogram and bar charts, and customized layouts. Performed POC to create\\nAI Agents to onboard new models to RT-AIMLOPs platform using VEGAS\\n(Verizon Enterprise GenAI & Agentic Services).\\nSenior Data Scientist\\nAugust 2024\\xa0-\\xa0Present\\xa0(1 year 5 months)\\nIrving, Texas, United States\\nProject - Personalization AI (PzAI)\\n● Built, trained and tested models using PzAI architecture (a novel LLM\\nbased recommender system ref paper published by our team) for a number\\nof use cases such as dynamic offers, purchase propensity and channel\\nrecommendations \\n● Improved model performance by fine tuning models on large datasets using\\nA100/T4 systems on Domino Data Lab platform \\n● Wrote complex BigQuery SQLs for daily data ingestion and feeds to model\\ntraining \\n\\xa0 Page 1 of 6\\xa0 \\xa0\\n● Performed analytics on large datasets using PySpark to identify key features\\nand extract valuable insights relevant to use cases \\n● Implemented dynamic offer recommendation routing via Pega Adaptive\\nDecision Manager (ADM) \\n● Build postman testing suite using postman scripts to run 100s of test cases\\non multiple API endpoints \\n● Collaborated with cross-functional teams to RCA a number of defects and\\nfollowed through to closure \\n● Implemented model performance and drift monitoring in prod and retraining\\nstrategies. \\nProject - RT-AIMLOPs \\n● Setup monitoring for performance, XAI (Explainable Artificial Intelligence),\\ndrift, stability, operations and custom metrics for a number of ML and deep\\nlearning models in production \\n● Build a generic code template to generate all the common metrics such as\\nlift, R2, RMSE, accuracy, precision and recall \\n● Created script to perform SHAP (SHapley Additive exPlanations) analysis \\n● Extensively used Airflow, GCP dataproc, bigquery, Jenkins and Gitlab to set\\nup pipelines to load data \\n● Created multiple Grafana dashboards with timeseries, histogram and bar\\ncharts, and customized layouts \\n● Performed POC to create AI Agents to onboard new models to RT-AIMLOPs\\nplatform using VEGAS (Verizon Enterprise GenAI & Agentic Services)\\nAT&T\\nSenior Data Scientist\\nOctober 2023\\xa0-\\xa0December 2025\\xa0(2 years 3 months)\\nPlano, Texas, United States\\nFraud Detection using Knowledge Graph\\nCreated knowledge graphs for various fraud detection use cases using Rel\\nlanguage on Relational AI platform\\nPerformed EDA using PySpark, SQL and Python on large datasets from Azure\\nand Snowflake in databricks notebooks\\nDefined complex ontology models using Microsoft Visual Studio \\nGenerated graph export reports for number hijacking, post activation gaming\\nand data mining  fraud use cases\\nCVS Health\\nData Scientist\\n\\xa0 Page 2 of 6\\xa0 \\xa0\\nOctober 2021\\xa0-\\xa0June 2022\\xa0(9 months)\\nDallas, Texas, United States\\nSAGE IT, INC\\nData Scientist\\nOctober 2013\\xa0-\\xa0June 2022\\xa0(8 years 9 months)\\nFrisco, Texas\\nCVS\\nSenior Data Scientist\\nJanuary 2022\\xa0-\\xa0May 2022\\xa0(5 months)\\nDeveloped Generalized Machine Learning Pipeline (GMP) in PySpark that\\ningests hive data and automatically does data cleaning, feature selection,\\nmodel selection and tuning. Implemented a1c (target variable) missing\\nvalue imputation using Multivariate Imputation by chained equations (MICE)\\nalgorithm. Developed deep learning models via Keras using Rx, Dx data to\\npredict a1c. Co-ordinated development of a ML model to diagnose Asthma/\\nCOPD in collaboration with an external client (Novartis). Built complex hive\\nqueries to collect required data from multiple sources. Developed clinical study\\ndesign to test the ML model at CVS health hub locations.\\nVerizon\\n2 years 7 months\\nSenior Data Scientist\\nJune 2019\\xa0-\\xa0December 2021\\xa0(2 years 7 months)\\nUnited States\\nAnalyzed user sessions data via clustering to auto-detect browsing patterns\\n(workflows). Collate page URLs by user sessions for each application using\\nNew Relic API. Preprocess URLs via regex and convert URLs into bag\\nof words using TF-IDF vectorization. Perform clustering using DBSCAN,\\nOPTICS, K-Means. Segment user base for each application and identify\\npotential issues/opportunities based on volume of partial workflows. Extract\\nNew Relic RUM data using NRQL (New Relic Query Language) for over\\n100 applications. Extract Catchpoint data using custom python script. Wrote\\ncomplex regex to preprocess URLs and perform correlation, and store data\\nin Postgres tables. Build tableau dashboard with custom visualizations (Venn\\ndiagram/Diverging bar chart). Build a new relic application (Nerdpack) to\\npresent health of databases via custom overall score and by segment such as\\nInfrastructure, Application, Performance, Stability and Security. Wrote python\\nscripts to query multiple New Relic entities to get metrics values and calculate\\n\\xa0 Page 3 of 6\\xa0 \\xa0\\ncustom scores. Wrote complex NRQL queries to provide derived metrics and\\nrecommendations such as Top long running queries, Blocking parent/child\\nqueries.\\nLead Data Scientist\\nJune 2019\\xa0-\\xa0October 2021\\xa0(2 years 5 months)\\nIrving, Texas, United States\\n• Product owner of multiple data science/analytics projects actively contributing\\nend to end including business requirement analysis, technical stack\\nidentification, complete solution & workflow design, POC demos, development,\\nproduction support, and project management\\n• Proven track record in deriving business intelligence from complex data and\\ndelivering outstanding business value\\n• Designed end to end solution for detection of major network outages across\\nVerizon networks around the globe for project Verizon Intelligent Outage\\nDetection\\n• Orchestrated multiple rounds of data analysis and deep learning model\\nbuilding for the creation of the model with highest possible accuracy for project\\nManaged Services Automated Repair Controller\\nAT&T\\nData Scientist\\nDecember 2013\\xa0-\\xa0April 2019\\xa0(5 years 5 months)\\nDallas, Texas, United States\\n• Developed models to identify potential customers for new/related products\\nbased on user portal interactions, usage of product features, ordering history,\\nby applying various classification algorithms such as multiple regression\\n(linear/logistic), XGBoost, Deep Neural Network (Keras/PyTorch) \\n• Worked on more than 30 sprints of agile/SAFe project management.\\n• Provided end to end solution and workflows for interaction with enterprise\\nclients\\n• Generated business process models, sequence diagrams, use case\\ndiagrams using Enterprise Architect \\n• Designed large scale APIs and models following OAS3.0 (Open API\\nSpecification) in SwaggerHub\\n• Performed data mapping across multiple interfaces generating\\ncomprehensive spreadsheets to maintain data integrity and provide one stop\\nreference\\n• Regularly generated detailed documentation (Design documents, PPTs,\\nEpics and User stories in Rally) of design solutions, system requirements/\\nrestrictions for development teams enabling accurate implementation\\n\\xa0 Page 4 of 6\\xa0 \\xa0\\nMichigan Technological University\\nGraduate Research Assistant\\nSeptember 2009\\xa0-\\xa0June 2013\\xa0(3 years 10 months)\\nHoughton, Michigan, United States\\n• Extensively worked on large volumes of gene expression data from disparate\\nsources (Microarrays, Next-gen sequencers)\\n• Regularly performed data wrangling (cleaning, normalization, transformation\\nand mining) and exploratory data analysis (EDA) activities\\n• Analyzed and classified multiple biological conditions using machine learning\\ntechniques PCA, PLS-DA, R-SVM and RF\\n• Published three highly cited research articles in top journals based on above\\nwork (Ref1, Ref2, Ref3, Ref4)\\n• Developed a custom web application to query and display curated data using\\nJAVA, PhP and SQL\\n• Performed modular analysis of gene expression data using the R package\\nand identified expression patterns of drought and bacterial stress in rice\\nPhilips\\nScientist\\nDecember 2006\\xa0-\\xa0December 2008\\xa0(2 years 1 month)\\nPhilips Research Asia\\nScientist\\nNovember 2006\\xa0-\\xa0November 2008\\xa0(2 years 1 month)\\nMethylation prediction of CpG islands in different tissues for bio-marker\\ndiscovery using Perl and shell scripting. Developed enhanced algorithms for\\nprobe and primer sequence designs, utilizing SVM (Support Vector Machines)\\nfor probe classification and SVM-RFE for feature extraction (Ref). Filed a\\npatent on data mining system to analyze family genetic history and predict\\ndisease conditions (Ref).\\nEducation\\nMichigan Technological University\\nDoctor of Philosophy (Ph.D.),\\xa0Bioinformatics\\xa0·\\xa0(2009\\xa0-\\xa02013)\\nAndhra University\\nMaster of Science (M.Sc.),\\xa0Human/Medical Genetics\\xa0·\\xa0(2003\\xa0-\\xa02005)\\n\\xa0 Page 5 of 6\\xa0 \\xa0\\nAndhra University\\nMaster of Science,\\xa0Human/Medical Genetics\\nIBAB\\nPostgraduate Degree,\\xa0Bioinformatics\\nMichigan Technological University\\nDoctor of Philosophy,\\xa0Bioinformatics\\n\\xa0 Page 6 of 6\\n\\nWith this context, please chat with the user, always staying in character as Rafi Shaik.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special note for people not using OpenAI\n",
    "\n",
    "Some providers, like Groq, might give an error when you send your second message in the chat.\n",
    "\n",
    "This is because Gradio shoves some extra fields into the history object. OpenAI doesn't mind; but some other models complain.\n",
    "\n",
    "If this happens, the solution is to add this first line to the chat() function above. It cleans up the history variable:\n",
    "\n",
    "```python\n",
    "history = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "```\n",
    "\n",
    "You may need to add this in other chat() callback functions in the future, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A lot is about to happen...\n",
    "\n",
    "1. Be able to ask an LLM to evaluate an answer\n",
    "2. Be able to rerun if the answer fails evaluation\n",
    "3. Put this together into 1 workflow\n",
    "\n",
    "All without any Agentic framework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pydantic model for the Evaluation\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable. \\\n",
    "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \\\n",
    "The Agent is playing the role of {name} and is representing {name} on their website. \\\n",
    "The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "The Agent has been provided with context on {name} in the form of their summary and LinkedIn details. Here's the information:\"\n",
    "\n",
    "evaluator_system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "evaluator_system_prompt += f\"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_user_prompt(reply, message, history):\n",
    "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
    "    user_prompt += \"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gemini = OpenAI(\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\"), \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(reply, message, history) -> Evaluation:\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}]\n",
    "    response = gemini.beta.chat.completions.parse(model=\"gemini-2.0-flash\", messages=messages, response_format=Evaluation)\n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": system_prompt}] + [{\"role\": \"user\", \"content\": \"do you hold a patent?\"}]\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "reply = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, I hold a patent for a \"system method and computer program product for pedigree analysis.\" This patent reflects my work in developing innovative methods for analyzing genetic data, which is a key aspect of my background in bioinformatics. If you have any specific questions about the patent or related topics, feel free to ask!'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Evaluation(is_acceptable=True, feedback='The response is great. The agent accurately identifies the patent and offers more information. The tone is professional and engaging.')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(reply, \"do you hold a patent?\", messages[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun(reply, message, history, feedback):\n",
    "    updated_system_prompt = system_prompt + \"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
    "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
    "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    if \"patent\" in message:\n",
    "        system = system_prompt + \"\\n\\nEverything in your reply needs to be in pig latin - \\\n",
    "              it is mandatory that you respond only and entirely in pig latin\"\n",
    "    else:\n",
    "        system = system_prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    reply =response.choices[0].message.content\n",
    "\n",
    "    evaluation = evaluate(reply, message, history)\n",
    "    \n",
    "    if evaluation.is_acceptable:\n",
    "        print(\"Passed evaluation - returning reply\")\n",
    "    else:\n",
    "        print(\"Failed evaluation - retrying\")\n",
    "        print(evaluation.feedback)\n",
    "        reply = rerun(reply, message, history, evaluation.feedback)       \n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed evaluation - returning reply\n",
      "Passed evaluation - returning reply\n",
      "Failed evaluation - retrying\n",
      "This response is not acceptable because it is in pig latin. While it references the patent listed in the provided information, the main goal of the persona is to represent Rafi Shaik in a professional, engaging way. The use of pig latin makes the response not professional and is jarring to the user.\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
